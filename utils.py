import pandas as pd
import requests
import re
from io import BytesIO
from sentence_transformers import SentenceTransformer, util
from nltk.stem.snowball import SnowballStemmer

# –ú–æ–¥–µ–ª—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# –°—Ç–µ–º–º–µ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
stemmer = SnowballStemmer("russian")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤
SYNONYM_GROUPS = [
    ["—Å–∏–º", "—Å–∏–º–∫–∞", "—Å–∏–º–∫–∞—Ä—Ç–∞", "—Å–∏–º-–∫–∞—Ä—Ç–∞", "—Å–∏–º-–∫–∞—Ä—Ç–µ", "—Å–∏–º–∫–µ", "—Å–∏–º–∫—É", "—Å–∏–º–∫–∏"],
    ["–∫—Ä–µ–¥–∏—Ç–∫–∞", "–∫—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞", "–∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∫–∞—Ä—Ç–æ–π", "–∫–∞—Ä—Ç–æ–π"],
    ["–Ω–∞–ª–∏—á–Ω—ã–µ", "–Ω–∞–ª–∏—á–∫–∞", "–Ω–∞–ª–∏—á–Ω—ã–º–∏"]
]

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–∑–∞–∏–º–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤ (–±—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø)
SYNONYM_DICT = {}
for group in SYNONYM_GROUPS:
    for word in group:
        stem = stemmer.stem(word.lower())
        SYNONYM_DICT[stem] = {stemmer.stem(w) for w in group}

# –°—Å—ã–ª–∫–∏ –Ω–∞ Excel-—Ñ–∞–π–ª—ã
GITHUB_CSV_URLS = [
    "https://raw.githubusercontent.com/skatzrsk/semantic-assistant/main/data1.xlsx",
    "https://raw.githubusercontent.com/skatzrsk/semantic-assistant/main/data2.xlsx",
    "https://raw.githubusercontent.com/skatzrsk/semantic-assistant/main/data3.xlsx"
]

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏
def preprocess(text):
    text = str(text).lower().strip()
    text = re.sub(r"\s+", " ", text)
    return text

# –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –Ω–∞ –ø–æ–¥—Ñ—Ä–∞–∑—ã –ø–æ /
def split_by_slash(phrase):
    parts = [p.strip() for p in str(phrase).split("/") if p.strip()]
    return parts if len(parts) > 1 else [phrase]

# –ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ Excel-—Ñ–∞–π–ª–∞ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ /
def load_excel(url):
    response = requests.get(url)
    if response.status_code != 200:
        raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}")
    df = pd.read_excel(BytesIO(response.content))

    topic_cols = [col for col in df.columns if col.lower().startswith("topics")]
    if not topic_cols:
        raise KeyError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ topics")

    rows = []
    for _, row in df.iterrows():
        phrase = row['phrase']
        topics = [t for t in row[topic_cols].fillna('').tolist() if t]
        for sub_phrase in split_by_slash(phrase):
            rows.append({
                'phrase': sub_phrase,
                'phrase_proc': preprocess(sub_phrase),
                'phrase_full': phrase,  # –Ω–æ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                'topics': topics
            })

    return pd.DataFrame(rows)

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö Excel-—Ñ–∞–π–ª–æ–≤
def load_all_excels():
    dfs = []
    for url in GITHUB_CSV_URLS:
        try:
            df = load_excel(url)
            dfs.append(df)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å {url}: {e}")
    if not dfs:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
    return pd.concat(dfs, ignore_index=True)

# –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
def semantic_search(query, df, top_k=5, threshold=0.5):
    query_proc = preprocess(query)
    query_emb = model.encode(query_proc, convert_to_tensor=True)
    phrase_embs = model.encode(df['phrase_proc'].tolist(), convert_to_tensor=True)

    sims = util.pytorch_cos_sim(query_emb, phrase_embs)[0]
    results = []

    for idx, score in enumerate(sims):
        score = float(score)
        if score >= threshold:
            phrase_full = df.iloc[idx]['phrase_full']
            topics = df.iloc[idx]['topics']
            results.append((score, phrase_full, topics))

    results.sort(key=lambda x: x[0], reverse=True)
    return results[:top_k]

# –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö —Å–ª–æ–≤ –∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
def keyword_search(query, df):
    query_proc = preprocess(query)
    query_words = re.findall(r"\w+", query_proc)
    query_stems = [stemmer.stem(word) for word in query_words]

    matched = []
    for _, row in df.iterrows():
        phrase_words = re.findall(r"\w+", row['phrase_proc'])
        phrase_stems = {stemmer.stem(word) for word in phrase_words}

        # –í—Å–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ (–∏–ª–∏ –∏—Ö —Å–∏–Ω–æ–Ω–∏–º—ã) –¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤ —Ñ—Ä–∞–∑–µ
        if all(
            any(
                qs in SYNONYM_DICT.get(ps, {ps})
                for ps in phrase_stems
            )
            for qs in query_stems
        ):
            matched.append((row['phrase_full'], row['topics']))

    return matched

# üìå –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ç–µ–º–∞—Ç–∏–∫–∞–º
def filter_by_topics(results, selected_topics):
    if not selected_topics:
        return results

    filtered = []
    for item in results:
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤: (score, phrase, topics) –∏–ª–∏ (phrase, topics)
        if isinstance(item, tuple) and len(item) == 3:
            score, phrase, topics = item
            filtered.append((score, phrase, topics))
        elif isinstance(item, tuple) and len(item) == 2:
            phrase, topics = item
            filtered.append((phrase, topics))
        else:
            continue

    # –û—Ç–±–æ—Ä: —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ —Ç–µ–º–∞ –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç
    return [
        item for item in filtered
        if any(topic in item[-1] for topic in selected_topics)
    ]

