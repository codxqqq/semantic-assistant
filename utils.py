import pandas as pd
import requests
import re
from io import BytesIO
from sentence_transformers import SentenceTransformer, util
import pymorphy2
import functools
import torch

# ‚ö° –õ–µ–Ω–∏–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
@functools.lru_cache(maxsize=1)
def get_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# ‚ö° –õ–µ–Ω–∏–≤—ã–π –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä
@functools.lru_cache(maxsize=1)
def get_morph():
    return pymorphy2.MorphAnalyzer()

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏
def preprocess(text):
    text = str(text).lower().strip()
    text = re.sub(r"\s+", " ", text)
    return text

# –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
def lemmatize(word):
    return get_morph().parse(word)[0].normal_form

# ‚úÖ –ö—ç—à–∏—Ä—É–µ–º–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
@functools.lru_cache(maxsize=10000)
def lemmatize_cached(word):
    return lemmatize(word)

# –°–∏–Ω–æ–Ω–∏–º–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã
SYNONYM_GROUPS = [
    ["—Å–∏–º", "—Å–∏–º–∫–∞", "—Å–∏–º–∫–∞—Ä—Ç–∞", "—Å–∏–º-–∫–∞—Ä—Ç–∞", "—Å–∏–º-–∫–∞—Ä—Ç–µ", "—Å–∏–º–∫–µ", "—Å–∏–º–∫—É", "—Å–∏–º–∫–∏"],
    ["–∫—Ä–µ–¥–∏—Ç–∫–∞", "–∫—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞", "–∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∫–∞—Ä—Ç–æ–π", "–∫–∞—Ä—Ç–æ–π"],
    ["–Ω–∞–ª–∏—á–Ω—ã–µ", "–Ω–∞–ª–∏—á–∫–∞", "–Ω–∞–ª–∏—á–Ω—ã–º–∏"]
]

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤
SYNONYM_DICT = {}
for group in SYNONYM_GROUPS:
    lemmas = {lemmatize(w.lower()) for w in group}
    for lemma in lemmas:
        SYNONYM_DICT[lemma] = lemmas

# –°—Å—ã–ª–∫–∏ –Ω–∞ Excel-—Ñ–∞–π–ª—ã
GITHUB_CSV_URLS = [
    "https://raw.githubusercontent.com/skatzrsk/semantic-assistant/main/data1.xlsx",
    "https://raw.githubusercontent.com/skatzrsk/semantic-assistant/main/data2.xlsx",
    "https://raw.githubusercontent.com/skatzrsk/semantic-assistant/main/data3.xlsx"
]

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ñ—Ä–∞–∑ –ø–æ /
def split_by_slash(phrase):
    parts = [p.strip() for p in str(phrase).split("/") if p.strip()]
    return parts if parts else [phrase]

# ‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ Excel-—Ñ–∞–π–ª–∞
def load_excel(url):
    response = requests.get(url)
    if response.status_code != 200:
        raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}")
    df = pd.read_excel(BytesIO(response.content))

    topic_cols = [col for col in df.columns if col.lower().startswith("topics")]
    if not topic_cols:
        raise KeyError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ topics")

    df['topics'] = df[topic_cols].astype(str).agg(lambda x: [v for v in x if v and v != 'nan'], axis=1)
    df['phrase_full'] = df['phrase']
    df['phrase_list'] = df['phrase'].apply(split_by_slash)
    df = df.explode('phrase_list', ignore_index=True)
    df['phrase'] = df['phrase_list']
    df['phrase_proc'] = df['phrase'].apply(preprocess)
    return df[['phrase', 'phrase_proc', 'phrase_full', 'topics']]

# üîÅ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
def compute_embeddings(df):
    model = get_model()
    df['embedding'] = model.encode(df['phrase_proc'].tolist(), convert_to_tensor=True)
    return df

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö Excel-—Ñ–∞–π–ª–æ–≤
def load_all_excels():
    dfs = []
    for url in GITHUB_CSV_URLS:
        try:
            dfs.append(load_excel(url))
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å {url}: {e}")
    if not dfs:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
    df = pd.concat(dfs, ignore_index=True)
    return compute_embeddings(df)

# ‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
def semantic_search(query, df, top_k=5, threshold=0.5):
    model = get_model()
    query_proc = preprocess(query)
    query_emb = model.encode(query_proc, convert_to_tensor=True)

    sims = util.pytorch_cos_sim(query_emb, torch.stack(df['embedding'].tolist()))[0]
    results = [
        (float(score), df.iloc[idx]['phrase_full'], df.iloc[idx]['topics'])
        for idx, score in enumerate(sims) if float(score) >= threshold
    ]
    return sorted(results, key=lambda x: x[0], reverse=True)[:top_k]

# ‚úÖ –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —á–µ—Ä–µ–∑ apply)
def keyword_search(query, df):
    query_proc = preprocess(query)
    query_words = re.findall(r"\w+", query_proc)
    query_lemmas = [lemmatize_cached(word) for word in query_words]

    def match_row(row):
        phrase_words = re.findall(r"\w+", row.phrase_proc)
        phrase_lemmas = {lemmatize_cached(word) for word in phrase_words}
        return all(
            any(ql in SYNONYM_DICT.get(pl, {pl}) for pl in phrase_lemmas)
            for ql in query_lemmas
        )

    matches = df[df.apply(match_row, axis=1)]
    return list(zip(matches['phrase'], matches['topics']))
